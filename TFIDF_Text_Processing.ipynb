{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehGs8135ho2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from scipy.sparse import csr_matrix\n",
        "import math\n",
        "import operator\n",
        "from sklearn.preprocessing import normalize\n",
        "import numpy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9x_doOYiY7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = [\n",
        "     'this is the first document',\n",
        "     'this document is the second document',\n",
        "     'and this is the third one',\n",
        "     'is this the first document',\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR4fjmpwiKxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(dataset):    \n",
        "    unique_words = set() # at first we will initialize an empty set\n",
        "    # check if its list type or not\n",
        "    if isinstance(dataset, (list,)):\n",
        "        for row in dataset: # for each review in the dataset\n",
        "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
        "                if len(word) < 2:\n",
        "                    continue\n",
        "                unique_words.add(word)\n",
        "        unique_words = sorted(list(unique_words))\n",
        "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "        return vocab\n",
        "    else:\n",
        "        print(\"you need to pass list of sentance\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_9DOCiKiQLR",
        "colab_type": "code",
        "outputId": "3286a519-dfa1-465b-c7bc-aa9760cccf7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = fit(corpus)\n",
        "print(vocab)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 0, 'document': 1, 'first': 2, 'is': 3, 'one': 4, 'second': 5, 'the': 6, 'third': 7, 'this': 8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPpcfXEzixGW",
        "colab_type": "code",
        "outputId": "90d20186-f970-4dbf-bde1-072482f5b9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Unique Words are \n",
        "print(list(vocab.keys()))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTrr5_LQjOCA",
        "colab_type": "text"
      },
      "source": [
        "Calculate IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0PQ1Iibjrq-",
        "colab_type": "code",
        "outputId": "fe993517-5539-4c1e-951a-53241564b046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "n_documents = len(corpus)\n",
        "print(n_documents)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyazuWsLjc9X",
        "colab_type": "code",
        "outputId": "f8ec2106-9bc0-42ce-9ce2-6b9b29e1c03d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        " \n",
        "def transform(dataset,vocab):\n",
        "  #Calculating IDF\n",
        "  idf_={}\n",
        "  tfdic=[]\n",
        "  di={}\n",
        "  for d in range (len(dataset)):\n",
        "    doc=list(dataset[d].split())\n",
        "    for word in doc:\n",
        "      if word in vocab.keys():\n",
        "        tf_cnt = doc.count(word)\n",
        "      \n",
        "      tf = (tf_cnt/len(doc))\n",
        "      di.update({word:tf})\n",
        "    \n",
        "    tfdic.append(di)\n",
        "    di={}\n",
        "     \n",
        "  \n",
        "  for word in vocab:\n",
        "      count = 0\n",
        "      idf=0\n",
        "      for doc in dataset:\n",
        "        if word in doc:\n",
        "          count = count +1\n",
        "      idf=1+(math.log((1+n_documents)/(1+count)))   \n",
        "      idf_[word]=idf\n",
        "  rows = []\n",
        "  columns = []\n",
        "  values=[]\n",
        "  l1=[]\n",
        "  for idx, row in enumerate(tqdm(dataset)): #Loop through the corpus\n",
        "    l1=list(row.split(\" \")) #Get all the words for current row\n",
        "    for word in l1:\n",
        "      col_index = vocab.get(word, -1)#Find the index of word in vocab\n",
        "      if col_index !=-1:\n",
        "        \n",
        "        df=idf_.get(word,-1)#Get the IDF value of the word\n",
        "        tf1=tfdic[idx] #Getting the TF scores corresponding to the document\n",
        "        tf2=tf1.get(word,-1)#Getting TF value\n",
        "        val=tf2*df#Calculating TF-IDF\n",
        "        # we are storing the dimensions of the word\n",
        "        rows.append(idx)\n",
        "        # we are storing the index of the document\n",
        "        columns.append(col_index)           \n",
        "        values.append(val)\n",
        "  return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab))) \n",
        "\n",
        "print(transform(corpus,vocab).toarray())\n",
        "X=transform(corpus,vocab)\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00, 3856.83it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00, 2426.91it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.24462871 0.30216512 0.2        0.         0.\n",
            "  0.2        0.         0.2       ]\n",
            " [0.         0.81542903 0.         0.16666667 0.         0.31938179\n",
            "  0.16666667 0.         0.16666667]\n",
            " [0.31938179 0.         0.         0.16666667 0.31938179 0.\n",
            "  0.16666667 0.31938179 0.16666667]\n",
            " [0.         0.24462871 0.30216512 0.2        0.         0.\n",
            "  0.2        0.         0.2       ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRd1atv1EWdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nl2=normalize(X, norm='l2', axis=1, copy=True, return_norm=False)#L2 normalisation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN4kaIYcFCk7",
        "colab_type": "code",
        "outputId": "ce3a20fc-e739-43f7-af03-418390b2924a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "nl2.shape #Rows in corpus and Columns are vocab"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC_53GpFEJnj",
        "colab_type": "code",
        "outputId": "628b67f6-570c-4df3-bf2a-6170f73ab4b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(nl2[0]) #Document1 sparse matrix"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1)\t0.4697913855799205\n",
            "  (0, 2)\t0.580285823684436\n",
            "  (0, 3)\t0.3840852409148149\n",
            "  (0, 6)\t0.3840852409148149\n",
            "  (0, 8)\t0.3840852409148149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0ke-8G3LI-A",
        "colab_type": "code",
        "outputId": "be4ac743-50ba-4d05-f370-77bba06d7ee5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(nl2[0].toarray()) #Dense Matrix"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
            "  0.38408524 0.         0.38408524]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu3YNYOfPv12",
        "colab_type": "code",
        "outputId": "df1d92da-e338-483c-8c21-e471388e28ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "print(nl2)#Displaying whole Sparse Matrix"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 1)\t0.4697913855799205\n",
            "  (0, 2)\t0.580285823684436\n",
            "  (0, 3)\t0.3840852409148149\n",
            "  (0, 6)\t0.3840852409148149\n",
            "  (0, 8)\t0.3840852409148149\n",
            "  (1, 1)\t0.8843203931656719\n",
            "  (1, 3)\t0.18074746668441155\n",
            "  (1, 5)\t0.3463646952170705\n",
            "  (1, 6)\t0.18074746668441155\n",
            "  (1, 8)\t0.18074746668441155\n",
            "  (2, 0)\t0.511848512707169\n",
            "  (2, 3)\t0.267103787642168\n",
            "  (2, 4)\t0.511848512707169\n",
            "  (2, 6)\t0.267103787642168\n",
            "  (2, 7)\t0.511848512707169\n",
            "  (2, 8)\t0.267103787642168\n",
            "  (3, 1)\t0.4697913855799205\n",
            "  (3, 2)\t0.580285823684436\n",
            "  (3, 3)\t0.3840852409148149\n",
            "  (3, 6)\t0.3840852409148149\n",
            "  (3, 8)\t0.3840852409148149\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxZU867_YoGk",
        "colab_type": "text"
      },
      "source": [
        "#TASK 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-unVwIRYkb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aeba0696-abbd-46f9-cc1e-1c710cba9acc"
      },
      "source": [
        "import pickle\n",
        "with open('cleaned_strings', 'rb') as f:\n",
        "    corpus = pickle.load(f)\n",
        "    \n",
        "# printing the length of the corpus loaded\n",
        "print(\"Number of documents in corpus = \",len(corpus))"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of documents in corpus =  746\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt66EfjjYyE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(dataset):\n",
        "    unique_words = set() # at first we will initialize an empty set\n",
        "    # check if its list type or not\n",
        "    if isinstance(dataset, (list,)):\n",
        "        for row in dataset: # for each review in the dataset\n",
        "            for word in row.split(\" \"): # for each word in the review. #split method converts a string into list of words\n",
        "                if len(word) < 2:\n",
        "                    continue\n",
        "                unique_words.add(word)\n",
        "        unique_words = sorted(list(unique_words))\n",
        "        vocab = {j:i for i,j in enumerate(unique_words)}\n",
        "        return vocab\n",
        "    else:\n",
        "        print(\"you need to pass list of sentance\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1xq19T5Y2Ig",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = fit(corpus)\n",
        "n_documents = len(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqkbyp4wZAkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform(dataset,vocab):\n",
        "  unique= []\n",
        "  tf_={}\n",
        "  idf_={}\n",
        "  tfdic=[]\n",
        "  di={}\n",
        "  \n",
        "  for word in vocab:\n",
        "      count = 0\n",
        "      idf=0\n",
        "      for doc in dataset:\n",
        "        if word in doc:\n",
        "          count = count +1\n",
        "      idf=1+(math.log((1+n_documents)/(1+count)))   \n",
        "      idf_[word]=idf\n",
        "      \n",
        "  #Sorting the IDF Dictionary based on top IDF values\n",
        "  l=list(idf_.items())\n",
        "  l.sort(reverse=True)\n",
        "  idf_=dict(l)\n",
        "  idf_50={}\n",
        "  cnt=0\n",
        "  vocab_50={}\n",
        "  #Generating top 50 vocab and IDF\n",
        "  for key,value in idf_.items():\n",
        "    if cnt<50:\n",
        "      idf_50[key]=value\n",
        "      unique.append(key)\n",
        "      unique = sorted(list(unique))\n",
        "      vocab_50 = {j:i for i,j in enumerate(unique)}\n",
        "      cnt = cnt+1\n",
        "  print(\"Top Features and IDF Scores\")\n",
        "  for item in idf_50.items():\n",
        "    print(item)\n",
        "  \n",
        "  #Generating a list of dictionary to store the TF values for each document\n",
        "  for d in range (len(dataset)):\n",
        "    doc=list(dataset[d].split())\n",
        "    for word in doc:\n",
        "      if word in vocab_50.keys():\n",
        "        tf_cnt = doc.count(word)\n",
        "        tf = (tf_cnt/len(doc)) \n",
        "        di.update({word:tf})\n",
        "    \n",
        "    tfdic.append(di)\n",
        "    di={}        \n",
        "  rows = []\n",
        "  columns = []\n",
        "  values=[]\n",
        "  l1=[]\n",
        "  for idx, row in enumerate(tqdm(dataset)):\n",
        "    l1=list(row.split(\" \"))\n",
        "    for word in l1:\n",
        "      col_index = vocab_50.get(word, -1)\n",
        "      if col_index !=-1:\n",
        "        # we are storing the index of the document\n",
        "        df=idf_50.get(word,-1)#Getting IDF value\n",
        "        tf1=tfdic[idx] #Getting the TF scores corresponding to the document\n",
        "        tf2=tf1.get(word,-1)#Getting TF value\n",
        "        val=tf2*df#Calculating TF-IDF\n",
        "        rows.append(idx)     \n",
        "        columns.append(col_index)            \n",
        "        values.append(val)\n",
        "\n",
        "  return csr_matrix((values, (rows,columns)), shape=(len(dataset),len(vocab_50))) \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBVLB7idZ9Ul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "239bfb95-5ff1-4beb-c33d-b3e32f64f625"
      },
      "source": [
        "#print(transform(corpus,vocab).toarray())\n",
        "X=transform(corpus,vocab)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 746/746 [00:00<00:00, 246471.11it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Top Features and IDF Scores\n",
            "('zombiez', 6.922918004572872)\n",
            "('zombie', 6.229770824012927)\n",
            "('zillion', 6.922918004572872)\n",
            "('yun', 6.922918004572872)\n",
            "('youtube', 6.922918004572872)\n",
            "('youthful', 6.922918004572872)\n",
            "('younger', 6.922918004572872)\n",
            "('young', 5.824305715904762)\n",
            "('yet', 5.824305715904762)\n",
            "('yes', 5.536623643452981)\n",
            "('yelps', 6.922918004572872)\n",
            "('years', 5.05111582767128)\n",
            "('year', 4.843476462893037)\n",
            "('yeah', 6.517452896464707)\n",
            "('yawn', 6.922918004572872)\n",
            "('yardley', 6.922918004572872)\n",
            "('wrote', 6.922918004572872)\n",
            "('wrong', 6.229770824012927)\n",
            "('written', 5.536623643452981)\n",
            "('writing', 4.9770078555175585)\n",
            "('writers', 6.922918004572872)\n",
            "('writer', 5.536623643452981)\n",
            "('write', 5.536623643452981)\n",
            "('wrap', 6.922918004572872)\n",
            "('wow', 6.922918004572872)\n",
            "('woven', 6.922918004572872)\n",
            "('wouldnt', 6.922918004572872)\n",
            "('would', 4.248769355146344)\n",
            "('worthy', 6.229770824012927)\n",
            "('worthwhile', 6.922918004572872)\n",
            "('worthless', 6.922918004572872)\n",
            "('worth', 4.620332911578826)\n",
            "('worst', 5.218169912334447)\n",
            "('worse', 5.218169912334447)\n",
            "('worry', 6.922918004572872)\n",
            "('world', 5.824305715904762)\n",
            "('works', 5.824305715904762)\n",
            "('working', 6.517452896464707)\n",
            "('worked', 6.922918004572872)\n",
            "('work', 4.320228319128488)\n",
            "('words', 6.006627272698717)\n",
            "('word', 5.3134800921387715)\n",
            "('wooden', 6.517452896464707)\n",
            "('woo', 5.824305715904762)\n",
            "('wont', 6.922918004572872)\n",
            "('wong', 6.922918004572872)\n",
            "('wonderfully', 6.229770824012927)\n",
            "('wonderful', 4.525022731774501)\n",
            "('wondered', 6.922918004572872)\n",
            "('wonder', 4.438011354784871)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5gKXOu1cYeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nl2=normalize(X, norm='l2', axis=1, copy=True, return_norm=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2BX3UEYeBp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fca4dc1-0023-47b1-c625-e4182a6b6585"
      },
      "source": [
        "nl2.shape"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(746, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbJYw53WeFFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0290b70-9d70-4c34-c989-af3a9774ec34"
      },
      "source": [
        "print(nl2[0])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 42)\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f46uIn7eIig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "d550a5b4-12eb-4d2b-ff2a-0185505f85f7"
      },
      "source": [
        "print(nl2)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 42)\t1.0\n",
            "  (5, 13)\t1.0\n",
            "  (19, 9)\t0.13340045056356076\n",
            "  (19, 10)\t0.3837896897173447\n",
            "  (19, 15)\t0.15375023937679422\n",
            "  (19, 17)\t0.11588969746575903\n",
            "  (19, 18)\t0.10261240862400675\n",
            "  (19, 22)\t0.8492435913311929\n",
            "  (19, 30)\t0.11053375883703274\n",
            "  (19, 31)\t0.12296219743724018\n",
            "  (19, 40)\t0.12296219743724018\n",
            "  (19, 44)\t0.15375023937679422\n",
            "  (20, 38)\t1.0\n",
            "  (21, 8)\t1.0\n",
            "  (26, 31)\t1.0\n",
            "  (29, 18)\t1.0\n",
            "  (45, 18)\t1.0\n",
            "  (48, 30)\t1.0\n",
            "  (49, 30)\t1.0\n",
            "  (62, 22)\t1.0\n",
            "  (66, 14)\t1.0\n",
            "  (68, 22)\t1.0\n",
            "  (79, 10)\t0.595755244213343\n",
            "  (79, 14)\t0.8031660407364719\n",
            "  (82, 18)\t1.0\n",
            "  :\t:\n",
            "  (644, 43)\t0.2467542120355904\n",
            "  (644, 48)\t0.22204830243926363\n",
            "  (654, 17)\t1.0\n",
            "  (658, 7)\t1.0\n",
            "  (661, 2)\t1.0\n",
            "  (674, 2)\t0.5168710901842515\n",
            "  (674, 31)\t0.6324212867344452\n",
            "  (674, 38)\t0.5769641169231339\n",
            "  (677, 10)\t1.0\n",
            "  (678, 2)\t1.0\n",
            "  (679, 2)\t1.0\n",
            "  (680, 20)\t1.0\n",
            "  (693, 41)\t1.0\n",
            "  (696, 28)\t0.7071067811865476\n",
            "  (696, 31)\t0.7071067811865476\n",
            "  (698, 41)\t1.0\n",
            "  (700, 22)\t1.0\n",
            "  (710, 28)\t1.0\n",
            "  (713, 18)\t1.0\n",
            "  (714, 3)\t1.0\n",
            "  (717, 3)\t1.0\n",
            "  (720, 19)\t1.0\n",
            "  (739, 2)\t1.0\n",
            "  (742, 10)\t1.0\n",
            "  (743, 8)\t1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}